@book{raschka_python_2015,
	title = {Python {Machine} {Learning}},
	isbn = {978-1-78355-513-0},
	abstract = {Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analyticsAbout This BookLeverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualizationLearn effective strategies and best practices to improve and optimize machine learning systems and algorithmsAsk – and answer – tough questions of your data with robust statistical models, built for a range of datasetsWho This Book Is ForIf you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning – whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource.What You Will LearnExplore how to use different machine learning models to ask different questions of your dataLearn how to build neural networks using Pylearn 2 and TheanoFind out how to write clean and elegant Python code that will optimize the strength of your algorithmsDiscover how to embed your machine learning model in a web application for increased accessibilityPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringOrganize data using effective pre-processing techniquesGet to grips with sentiment analysis to delve deeper into textual and social media dataIn DetailMachine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data – its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success.Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Pylearn2, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization.Style and approachPython Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.},
    url = {https://www.amazon.com/Python-Machine-Learning-Sebastian-Raschka/dp/1783555130/ref=sr_1_1?ie=UTF8&qid=1478543661&sr=8-1},
	language = {English},
	publisher = {Packt Publishing - ebooks Account},
	author = {Raschka, Sebastian},
	month = sep,
	year = {2015}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {0007-4985, 1522-9602},
	url = {http://link.springer.com/article/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2016-10-18},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	year = {1943},
	pages = {115--133},
	file = {Snapshot:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/62686PAM/BF02478259.html:text/html}
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html},
	doi = {10.1038/nature14236},
	abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
	language = {en},
	number = {7540},
	urldate = {2016-10-18},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	keywords = {Computer science},
	pages = {529--533},
	file = {Full Text PDF:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/3CPS3TK2/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf;Snapshot:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/EKAEDAQK/nature14236.html:text/html}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {© 1986 Nature Publishing Group},
	issn = {0028-0836},
	url = {http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2016-10-15},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
	file = {Snapshot:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/UTMGK8NS/323533a0.html:text/html}
}

@article{rosenblatt_perceptron_1957,
    title = {The perceptron, a perceiving and recognizing automaton},
    journal = {Project Para. Cornell Aeronautical Laborator},
    author = {Rosenblatt, F.},
    year = {1957},
    file =
    {rosenblatt-1957.pdf:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/UTFN62QR/rosenblatt-1957.pdf:application/pdf}
}

@article{rosenblatt_perceptron:_1958,
	title = {The {Perceptron}: {A} {Probabilistic} {Model} for {Information} {Storage} and {Organization} in {The} {Brain}},
	shorttitle = {The {Perceptron}},
	abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	pages = {65--386},
	file = {Citeseer - Full Text PDF:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/R7CC9B3U/Rosenblatt - 1958 - The Perceptron A Probabilistic Model for Informat.pdf:application/pdf;Citeseer - Snapshot:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/82AIEBKQ/summary.html:text/html}
}

@article{widrow_adapting_1960,
	title = {Adapting switching circuits},
	url = {http://www-isl.stanford.edu/people/widrow/papers/c1960adaptiveswitching.pdf},
	journal = {IRE WESCON Convention record},
	author = {Widrow, B. and Hoff, M.E.},
	year = {1960},
	pages = {96--104}
}

@article{turing_i.computing_1950,
	title = {I.—{COMPUTING} {MACHINERY} {AND} {INTELLIGENCE}},
	volume = {LIX},
	issn = {0026-4423, 1460-2113},
	url = {http://mind.oxfordjournals.org/cgi/doi/10.1093/mind/LIX.236.433},
	doi = {10.1093/mind/LIX.236.433},
	language = {en},
	number = {236},
	urldate = {2016-10-15},
	journal = {Mind},
	author = {Turing, A. M.},
	year = {1950},
	pages = {433--460},
	file = {Turing - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/4U45STVJ/Turing - 1950 - I.—COMPUTING MACHINERY AND INTELLIGENCE.pdf:application/pdf}
}

@book{bishop_pattern_2007,
	address = {New York},
	edition = {1st ed. 2006. Corr. 2nd printing 2011},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	isbn = {978-0-387-31073-2},
	url = {https://www.amazon.de/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=sr_1_1?ie=UTF8\&qid=1475680474\&sr=8-1\&keywords=Pattern+Recognition+and+Machine+Learning},
	language = {Englisch},
	publisher = {Springer},
	author = {Bishop, Christopher},
	year = {2007}
}

@misc{scipy_development_team_scipy_2016,
	title = {Scipy {Lecture} {Notes}},
	url = {http://www.scipy-lectures.org/index.html},
	urldate = {2016-10-05},
	author = {Scipy development team},
	year = {2016},
}

@misc{numpy_development_team_numpy_2016, 
    title = {Numpy documentation}, 
    url = {http://docs.scipy.org/doc/numpy/}, 
    urldate = {2016-10-05}, 
    author = {Numpy development team}, 
    year = {2016} 
}

@misc{matplotlib_development_team_matplotlib_2016, 
    title = {Matplotlib documentation}, 
    url = {http://matplotlib.org/contents.html}, 
    urldate = {2016-10-05}, 
    author = {Matplotlib development team}, 
    year = {2016} 
}

@misc{python_development_team_python_2016,
	title = {Python 3 {Documentation}},
	url = {https://docs.python.org/3/},
	urldate = {2016-10-05},
	author = {Python development team},
	year = {2016}
}


@book{mohri_foundations_2012,
	address = {Cambridge, MA},
	title = {Foundations of {Machine} {Learning}},
	isbn = {978-0-262-01825-8},
	url = {https://www.amazon.de/Foundations-Machine-Learning-Adaptive-Computation/dp/026201825X/ref=sr_1_fkmr0_1?ie=UTF8\&qid=1475680504\&sr=8-1-fkmr0\&keywords=Foundations+of+Machine+Learning++++33+..+\%5BNielson\%5D+Neural+Networks+and+Deep+Learning},
	abstract = {This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book.The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.},
	language = {Englisch},
	publisher = {Mit University Press Group Ltd},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2012}
}

@misc{nielsen_neural_2015,
	title = {Neural {Networks} and {Deep} {Learning}},
	url = {http://neuralnetworksanddeeplearning.com},
	urldate = {2016-10-05},
	author = {Nielsen, Michael A.},
	year = {2015},
	file = {Snapshot:/Users/desertfrog/Dropbox/eBooks/Zotero/storage/7MFEANX5/Nielsen - 2015 - Neural Networks and Deep Learning.html:text/html}
}

@book{russell_artificial_2010,
	address = {Upper Saddle River},
	edition = {00003},
	title = {Artificial {Intelligence}: {A} {Modern} {Approach}},
	isbn = {978-0-13-604259-4},
	shorttitle = {Artificial {Intelligence}},
	url = {https://www.amazon.de/Artificial-Intelligence-Modern-Approach-Prentice/dp/0136042597/ref=sr_1_1?ie=UTF8\&qid=1475680391\&sr=8-1\&keywords=russell\%2C+norvig\%3A+artificial+intelligence+a+modern+approach},
	abstract = {Artificial Intelligence: A Modern Approach, 3e offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Number one in its field, this textbook is ideal for one or two-semester, undergraduate or graduate-level courses in Artificial Intelligence.  Dr. Peter Norvig, contributing Artificial Intelligence author and Professor Sebastian Thrun, a Pearson author are offering a free online course at Stanford University on artificial intelligence.   According to an article in  The New York Times , the course on artificial intelligence is one of three being offered experimentally by the Stanford computer science department to extend technology knowledge and skills beyond this elite campus to the entire world. One of the other two courses, an introduction to database software, is being taught by Pearson author Dr. Jennifer Widom.    Artificial Intelligence: A Modern Approach, 3e is available to purchase as an eText for your Kindle, NOOK, and the iPhone(r)/iPad(r).    To learn more about the course on artificial intelligence, visit http: //www.ai-class.com. To read the full New York Times article, click here."},
	language = {Englisch},
	publisher = {Prentice Hall},
	author = {Russell, Stuart J. and Norvig, Peter},
	year = {2010}
}
