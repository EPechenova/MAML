class LossSVM:

    def __init__(self, l, X_train, Y_train):
        """
        Initialize the parameter `l` = weight for the diameter of the margin and instances to the training data.
        """

        self.l_ = l
        self.xi_ = X_train
        self.yi_ = Y_train
        
        return

    def val(self, w):
        """
        Computes a value of the loss function for a given weight vector.
        """
        
        self.margin = 1 - self.yi_ * ( np.dot(self.xi_, w[1:]) + w[0] )
 
        w_norm = 1/2 * self.l_ * np.sum( w[1:]**2 ) 
        margin_violations = np.where(self.margin >= 0, self.margin, 0) 
        margin_avg = margin_violations.sum() / len(self.xi_)

        return w_norm + margin_avg

    def diff(self, w):
        """
        Computes the derivative of the loss function for a given weight vector.
        """
        
        self.margin = 1 - self.yi_ * ( np.dot(self.xi_, w[1:]) + w[0] )
        
        w0_sub_diff1 = - self.yi_
        w0_sub_diffs = np.where(self.margin >= 0, w0_sub_diff1, 0) 
        w0_diffs_avg = w0_sub_diffs.sum() / len(self.xi_)

        w_sub_diff1 = - self.xi_ * self.yi_[:,np.newaxis]
        w_sub_diffs = np.where(self.margin[:,np.newaxis] >= 0, w_sub_diff1, 0) 
        w_diffs_avg = w_sub_diffs.sum(axis=0) / len(self.xi_)

        ret0 = np.array([ w0_diffs_avg ])
        ret_vec = self.l_ * w[1:] + w_diffs_avg

        return np.append(ret0, ret_vec)

def learn(self, X_train, Y_train, l=1.0, eta=0.01, epochs=1000):
        '''
        fit training data according to eta and n_iter
        and log the errors in errors_
        '''

        # we initialize two list, each for the misclassifications and the cost function
        self.train_errors_ = []
        self.train_loss_ = []

        # for all the epoch
        for _ in range(epochs):
            # classify the traning features
            Z = self.classify(X_train)
            # count the misqualifications for the logging
            err = 0
            for z, y in zip(Z, Y_train):
                err += int(z != y)
            # ans save them in the list for later use
            self.train_errors_.append(err)

            loss = LossSVM(l, X_train, Y_train)
            # compute loss for this epoch
            self.train_loss_.append( loss.val(self.w_) )  
            # compute gradient of loss function and with it the update for w
            delta_w = - eta * loss.diff(self.w_)
            # update the weights
            self.w_ += delta_w

        return     
